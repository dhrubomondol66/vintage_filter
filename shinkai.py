# -*- coding: utf-8 -*-
"""Shinkai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cshur0RFX7qIlOsUtaGyWu8u7ec66jB1
"""

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive')

import os

# Change the path to where you saved the AnimeGANv2 folder
cartoongan_folder = '/content/drive/MyDrive/AnimeGANv2/AnimeGANv2-master'
os.chdir(cartoongan_folder)

import os
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

checkpoint_dir = '/content/drive/MyDrive/AnimeGANv2/AnimeGANv2-master/checkpoint'
model_name = "Paprika"

print("="*70)
print("INVESTIGATING CHECKPOINT STRUCTURE")
print("="*70)

# 1. List all files in checkpoint directory
print("\n1. All files in checkpoint directory:")
!ls -lah {checkpoint_dir}

# 2. Check specific model files
print(f"\n2. Files related to {model_name}:")
!ls -lah {checkpoint_dir}/*{model_name}*

# 3. Check if checkpoint state file exists
checkpoint_state_file = f'{checkpoint_dir}/checkpoint'
if os.path.exists(checkpoint_state_file):
    print(f"\n3. Checkpoint state file exists. Content:")
    with open(checkpoint_state_file, 'r') as f:
        print(f.read())
else:
    print(f"\n3. No checkpoint state file found at {checkpoint_state_file}")

# 4. Check inside the directories
for item in os.listdir(checkpoint_dir):
    item_path = os.path.join(checkpoint_dir, item)
    if os.path.isdir(item_path) and model_name in item:
        print(f"\n4. Contents of directory '{item}':")
        !ls -lah {item_path}

# 5. Try to read checkpoint using TensorFlow
print("\n5. Attempting to read checkpoint with TensorFlow:")
from tensorflow.python.training import py_checkpoint_reader

# Try different paths
paths_to_try = [
    f'{checkpoint_dir}/AnimeGANv2_{model_name}_lsgan_300_300_1_0_50_1',
    f'{checkpoint_dir}/generator_{model_name}_weight',
]

for path in paths_to_try:
    try:
        reader = py_checkpoint_reader.NewCheckpointReader(path)
        print(f"\n✓ Successfully read checkpoint: {path}")
        print(f"  Variables found: {len(reader.get_variable_to_shape_map())}")

        # Show first 10 variables
        var_map = reader.get_variable_to_shape_map()
        print("\n  First 10 variables:")
        for i, (key, shape) in enumerate(list(var_map.items())[:10]):
            print(f"    {key}: {shape}")

        # This is the correct path!
        checkpoint_path = path
        break
    except Exception as e:
        print(f"\n✗ Failed to read {path}")
        print(f"  Error: {str(e)[:100]}")

# 6. Check for .index and .data files
print("\n6. Looking for .index and .data files:")
!find {checkpoint_dir} -name "*.index" -o -name "*.data*"

print("\n" + "="*70)
print("INVESTIGATION COMPLETE")
print("="*70)

# Alternative: Try to load using the .data and .index files directly
import glob

checkpoint_dir = '/content/drive/MyDrive/AnimeGANv2/AnimeGANv2-master/checkpoint'
model_name = "Paprika"

# Find all .index files
index_files = glob.glob(f'{checkpoint_dir}/**/*.index', recursive=True)
print("Found .index files:")
for f in index_files:
    if model_name in f:
        print(f"  {f}")
        # The checkpoint path is the file without .index
        checkpoint_path = f.replace('.index', '')
        print(f"  → Checkpoint path: {checkpoint_path}")

# If no .index files, look for .data files
if not index_files:
    data_files = glob.glob(f'{checkpoint_dir}/**/*.data*', recursive=True)
    print("\nFound .data files:")
    for f in data_files:
        if model_name in f:
            print(f"  {f}")
            # Extract checkpoint path
            checkpoint_path = f.split('.data')[0]
            print(f"  → Checkpoint path: {checkpoint_path}")

from google.colab import files
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import numpy as np
import cv2
from PIL import Image
import os

model_name = "Paprika"
checkpoint_dir = '/content/drive/MyDrive/AnimeGANv2/AnimeGANv2-master/checkpoint'
generator_weight_path = f'{checkpoint_dir}/generator_{model_name}_weight'

checkpoint_path = tf.train.latest_checkpoint(generator_weight_path)
print(f"Checkpoint path: {checkpoint_path}")

# Try matching the original AnimeGANv2 variable naming
def spectral_norm(w, iteration=1):
    w_shape = w.shape.as_list()
    w = tf.reshape(w, [-1, w_shape[-1]])

    u = tf.get_variable("u", [1, w_shape[-1]], initializer=tf.random_normal_initializer(), trainable=False)

    u_hat = u
    v_hat = None
    for i in range(iteration):
        v_ = tf.matmul(u_hat, tf.transpose(w))
        v_hat = tf.nn.l2_normalize(v_)

        u_ = tf.matmul(v_hat, w)
        u_hat = tf.nn.l2_normalize(u_)

    u_hat = tf.stop_gradient(u_hat)
    v_hat = tf.stop_gradient(v_hat)

    sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))

    with tf.control_dependencies([u.assign(u_hat)]):
        w_norm = w / sigma
        w_norm = tf.reshape(w_norm, w_shape)

    return w_norm

def Conv2D(x, channels, kernel=3, stride=1, pad=0, pad_type='zero', use_bias=True, sn=False, scope='conv_0'):
    with tf.variable_scope(scope):
        if pad > 0:
            h = x.get_shape().as_list()[1]
            if h % stride == 0:
                pad = pad * 2
            else:
                pad = max(kernel - (h % stride), 0)

            pad_top = pad // 2
            pad_bottom = pad - pad_top
            pad_left = pad // 2
            pad_right = pad - pad_left

            if pad_type == 'zero':
                x = tf.pad(x, [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]])
            if pad_type == 'reflect':
                x = tf.pad(x, [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]], mode='REFLECT')

        weight_init = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)

        if sn:
            w = tf.get_variable("kernel", shape=[kernel, kernel, x.get_shape()[-1], channels], initializer=weight_init)
            x = tf.nn.conv2d(input=x, filter=spectral_norm(w), strides=[1, stride, stride, 1], padding='VALID' if pad > 0 else 'SAME')
            if use_bias:
                bias = tf.get_variable("bias", [channels], initializer=tf.constant_initializer(0.0))
                x = tf.nn.bias_add(x, bias)
        else:
            x = tf.layers.conv2d(inputs=x, filters=channels,
                               kernel_size=kernel, kernel_initializer=weight_init,
                               strides=stride, use_bias=use_bias, name='conv')

        return x

# Let me try a simpler approach - just load the checkpoint directly without building the graph
# This will show us the variable structure

from tensorflow.python.training import py_checkpoint_reader

reader = py_checkpoint_reader.NewCheckpointReader(checkpoint_path)
var_map = reader.get_variable_to_shape_map()

print(f"\nCheckpoint variables (first 20):")
for i, key in enumerate(sorted(var_map.keys())[:20]):
    print(f"  {key}")

# Based on the variable names we see, we'll build the correct generator
# For now, let me try the most common AnimeGANv2 structure

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()  # Disable TensorFlow 2.x behavior to use 1.x

import numpy as np
import cv2
from PIL import Image, ImageEnhance, ImageFilter
import os
from google.colab import files
from IPython.display import display # Import display for Colab

model_name = "Shinkai"  # Shinkai style works best for realistic anime characters
checkpoint_dir = '/content/drive/MyDrive/AnimeGANv2/AnimeGANv2-master/checkpoint'
generator_weight_path = f'{checkpoint_dir}/generator_{model_name}_weight' # Corrected path to the weight directory

checkpoint_path = tf.train.latest_checkpoint(generator_weight_path) # Get the latest checkpoint file
meta_path = checkpoint_path + '.meta' # Construct meta path from the actual checkpoint file

print("="*70)
print(f"ANIME CHARACTER TRANSFORMER - Model: {model_name}")
print("Transforms photos into anime-style characters")
print("="*70)

# Create generator using low-level TF operations
def conv2d(x, output_channels, kernel_size, strides=1, padding='SAME', use_bias=True, name=None):
    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):
        input_channels = x.get_shape().as_list()[-1]
        kernel_shape = [kernel_size, kernel_size, input_channels, output_channels]

        kernel = tf.get_variable('kernel', kernel_shape,
                                initializer=tf.truncated_normal_initializer(stddev=0.02))

        conv = tf.nn.conv2d(x, kernel, [1, strides, strides, 1], padding=padding)

        if use_bias:
            bias = tf.get_variable('bias', [output_channels],
                                  initializer=tf.zeros_initializer())
            conv = tf.nn.bias_add(conv, bias)

        return conv

def instance_norm(x, name='instance_norm'):
    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):
        depth = x.get_shape()[3]
        scale = tf.get_variable('scale', [depth], initializer=tf.ones_initializer())
        offset = tf.get_variable('offset', [depth], initializer=tf.zeros_initializer())

        mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)
        epsilon = 1e-5
        normalized = (x - mean) / tf.sqrt(variance + epsilon)

        return scale * normalized + offset

def resblock(x, filters, name='resblock'):
    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):
        shortcut = x

        h = conv2d(x, filters, 3, name='conv1')
        h = instance_norm(h, name='in1')
        h = tf.nn.relu(h)

        h = conv2d(h, filters, 3, name='conv2')
        h = instance_norm(h, name='in2')

        return shortcut + h

def upsample(x, output_channels, name='upsample'):
    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):
        # Get dynamic shape
        shape = tf.shape(x)
        new_h = shape[1] * 2
        new_w = shape[2] * 2

        # Resize
        x = tf.image.resize_bilinear(x, [new_h, new_w], align_corners=True)

        # Conv
        x = conv2d(x, output_channels, 3, name='conv')

        return x

def generator(inputs, reuse=False):
    with tf.variable_scope('generator', reuse=reuse):
        # Encoder
        x = tf.pad(inputs, [[0,0],[3,3],[3,3],[0,0]], 'REFLECT')
        x = conv2d(x, 64, 7, padding='VALID', name='enc_conv1')
        x = instance_norm(x, 'enc_in1')
        x = tf.nn.relu(x)

        x = conv2d(x, 128, 3, strides=2, name='enc_conv2')
        x = conv2d(x, 128, 3, name='enc_conv2_1')
        x = instance_norm(x, 'enc_in2')
        x = tf.nn.relu(x)

        x = conv2d(x, 256, 3, strides=2, name='enc_conv3')
        x = conv2d(x, 256, 3, name='enc_conv3_1')
        x = instance_norm(x, 'enc_in3')
        x = tf.nn.relu(x)

        # ResBlocks
        for i in range(8):
            x = resblock(x, 256, name=f'resblock_{i}')

        # Decoder
        x = upsample(x, 128, name='dec_upsample1')
        x = conv2d(x, 128, 3, name='dec_conv1')
        x = instance_norm(x, 'dec_in1')
        x = tf.nn.relu(x)

        x = upsample(x, 64, name='dec_upsample2')
        x = conv2d(x, 64, 3, name='dec_conv2')
        x = instance_norm(x, 'dec_in2')
        x = tf.nn.relu(x)

        x = tf.pad(x, [[0,0],[3,3],[3,3],[0,0]], 'REFLECT')
        x = conv2d(x, 3, 7, padding='VALID', use_bias=False, name='dec_conv_out')
        output = tf.nn.tanh(x)

        return output
def apply_anime_face_transform(image):
    """Apply transformations to make faces more anime-like"""

    img_pil = Image.fromarray(image)

    # 1. Detect face (if possible) and apply specific transformations
    try:
        img_cv = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

        gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)

        if len(faces) > 0:
            print(f"✓ Detected {len(faces)} face(s)")
            for (x, y, w, h) in faces:
                face_region = image[y:y+h, x:x+w]
    except Exception as e:
        print(f"Face detection skipped: {e}")

    # 2. Smooth skin areas (anime has smooth skin)
    img_array = np.array(img_pil)
    smoothed = cv2.bilateralFilter(img_array, 15, 80, 80)

    # 3. Sharpen edges (anime has sharp lines)
    sharpened = Image.fromarray(smoothed)
    sharpened = sharpened.filter(ImageFilter.SHARPEN)
    sharpened = sharpened.filter(ImageFilter.SHARPEN)  # Apply twice for stronger effect

    return np.array(sharpened)
def enhance_anime_style(image):
    """Post-process to enhance anime character look"""

    img_pil = Image.fromarray(image)

    enhancer = ImageEnhance.Color(img_pil)
    img_pil = enhancer.enhance(1.5)

    enhancer = ImageEnhance.Contrast(img_pil)
    img_pil = enhancer.enhance(1.3)

    enhancer = ImageEnhance.Sharpness(img_pil)
    img_pil = enhancer.enhance(2.0)

    enhancer = ImageEnhance.Brightness(img_pil)
    img_pil = enhancer.enhance(1.1)

    return np.array(img_pil)
def create_anime_lines(image):
    """Create clean anime-style line art overlay"""

    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    blurred = cv2.GaussianBlur(gray, (5, 5), 0)
    edges = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C,
                                  cv2.THRESH_BINARY, 9, 2)

    edges = cv2.bitwise_not(edges)

    kernel = np.ones((2, 2), np.uint8)
    edges = cv2.erode(edges, kernel, iterations=1)

    edges_rgb = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)
    result = cv2.multiply(image.astype(float), edges_rgb.astype(float) / 255.0)
    result = np.clip(result, 0, 255).astype(np.uint8)

    return result
# Upload image
print("\nUpload your photo:")
uploaded = files.upload()

for filename in uploaded.keys():
    output_base = '/content/drive/MyDrive/Dataset/cartoon_output/' + filename[:-5]

    # Read image
    img_original = cv2.imread(filename)
    if img_original is None:
        raise ValueError(f"Could not read: {filename}")

    img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)
    print(f"\n✓ Input shape: {img_original.shape}")

    # Apply anime face transformations
    print("\n✓ Pre-processing for anime character style...")
    img_preprocessed = apply_anime_face_transform(img_original)

    # Normalize and process through AnimeGANv2
    img = img_preprocessed.astype(np.float32) / 127.5 - 1.0
    img = np.expand_dims(img, 0)

    # Load model and generate
    tf.reset_default_graph()

    with tf.Session() as sess:
        print("✓ Loading AnimeGAN model...")
        saver = tf.train.import_meta_graph(meta_path)
        saver.restore(sess, checkpoint_path)

        graph = tf.get_default_graph()

        # Find tensors dynamically
        input_tensor = None
        output_tensor = None

        possible_inputs = ['test:0', 'input:0', 'generator_input:0', 'Placeholder:0']
        possible_outputs = ['output:0', 'generator_output:0', 'fake:0', 'generated:0']

        for name in possible_inputs:
            try:
                input_tensor = graph.get_tensor_by_name(name)
                break
            except:
                continue

        for name in possible_outputs:
            try:
                output_tensor = graph.get_tensor_by_name(name)
                break
            except:
                continue

        if input_tensor is None or output_tensor is None:
            ops = graph.get_operations()
            placeholders = [op for op in ops if op.type == 'Placeholder']
            tanh_ops = [op for op in ops if op.type == 'Tanh']

            if placeholders and input_tensor is None:
                input_tensor = graph.get_tensor_by_name(placeholders[0].name + ':0')
            if tanh_ops and output_tensor is None:
                output_tensor = graph.get_tensor_by_name(tanh_ops[-1].name + ':0')

        if input_tensor is None or output_tensor is None:
            raise ValueError("Could not find input or output tensors in the graph.")

        # Run inference
        print("✓ Transforming into anime character...")
        result = sess.run(output_tensor, feed_dict={input_tensor: img})

        # Postprocess
        result = result.squeeze()
        result = (result + 1.0) / 2.0 * 255.0
        result = np.clip(result, 0, 255).astype(np.uint8)

        # Apply anime character enhancements
        print("✓ Enhancing anime character features...")

        # Version 1: Standard with enhancements
        result_enhanced = enhance_anime_style(result)

        # Version 2: With anime line art
        result_with_lines = create_anime_lines(result_enhanced)

        # Version 3: Maximum anime effect
        result_max = enhance_anime_style(result_with_lines)

        # Save all versions
        os.makedirs(os.path.dirname(output_base), exist_ok=True)

        Image.fromarray(result).save(f"{output_base}_basic.jpg")
        Image.fromarray(result_enhanced).save(f"{output_base}_enhanced.jpg")
        Image.fromarray(result_with_lines).save(f"{output_base}_with_lines.jpg")
        Image.fromarray(result_max).save(f"{output_base}_maximum.jpg")

        print(f"\n✓ SUCCESS! Saved 4 versions:")
        print(f"  1. Basic: {output_base}_basic.jpg")
        print(f"  2. Enhanced: {output_base}_enhanced.jpg")
        print(f"  3. With Lines: {output_base}_with_lines.jpg")
        print(f"  4. Maximum Anime: {output_base}_maximum.jpg")

        # Display all versions
        print("\n--- ORIGINAL ---")
        display(Image.fromarray(img_original))

        print("\n--- BASIC ANIME ---")
        display(Image.fromarray(result))

        print("\n--- ENHANCED ANIME CHARACTER ---")
        display(Image.fromarray(result_enhanced))

        print("\n--- WITH ANIME LINES ---")
        display(Image.fromarray(result_with_lines))

        print("\n--- MAXIMUM ANIME EFFECT ---")
        display(Image.fromarray(result_max))

        # Create comparison grid
        h, w = 400, 400 # Adjusted size for better display in Colab
        comparison = np.vstack([
            np.hstack([
                cv2.resize(img_original, (w, h)),
                cv2.resize(result, (w, h))
            ]),
            np.hstack([
                cv2.resize(result_enhanced, (w, h)),
                cv2.resize(result_max, (w, h))
            ])
        ])

        comparison_path = f"{output_base}_comparison_grid.jpg"
        Image.fromarray(comparison).save(comparison_path)

        print("\n--- COMPARISON GRID ---")
        print("(Top: Original | Basic | Bottom: Enhanced | Maximum)")
        display(Image.fromarray(comparison))

print("\n" + "="*70)
print("TIPS FOR BETTER ANIME CHARACTER RESULTS:")
print("  1. Use clear, well-lit photos of faces")
print("  2. Front-facing photos work best")
print("  3. Try 'Hayao' model for Ghibli-style characters")
print("  4. Try 'Shinkai' model for realistic anime style")
print("  5. Try 'Paprika' model for vibrant anime colors")
print("="*70)

from google.colab import files

# Example: Download .pth file (PyTorch model weights)
files.download('/content/drive/MyDrive/AnimeGANv2/AnimeGANv2-master/checkpoint/generator_Shinkai_weight')

# Example: Download TensorFlow .meta model file
# The previous path was incorrect as it pointed to a directory name with a .meta suffix.
# Correcting to an actual .meta file found in the 'generator_Paprika_weight' directory.
files.download('/content/drive/MyDrive/AnimeGANv2/AnimeGANv2-master/checkpoint/generator_Paprika_weight/Paprika-54.ckpt.meta')

# Example: Download TensorFlow checkpoint files
# Correcting to actual .index and .data files found in the 'generator_Paprika_weight' directory.
files.download('/content/drive/MyDrive/AnimeGANv2/AnimeGANv2-master/checkpoint/generator_Paprika_weight/Paprika-54.ckpt.index')
files.download('/content/drive/MyDrive/AnimeGANv2/AnimeGANv2-master/checkpoint/generator_Paprika_weight/Paprika-54.ckpt.data-00000-of-00001')

# Example: Download Jupyter notebook file
# files.download('/content/your_notebook_name.ipynb')  # Replace with your notebook name

import shutil

# Zip the directory you want to download
shutil.make_archive('/content/model_checkpoint', 'zip', '/content/drive/MyDrive/AnimeGANv2/AnimeGANv2-master/checkpoint')

# Now download the zip file
files.download('/content/model_checkpoint.zip')

